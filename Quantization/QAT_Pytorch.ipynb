{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training quick implementation using Pytorch\n",
    "\n",
    "This small and simple notebook will guide you to quantize a model using QAT with only a few more line of code.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base class for the model\n",
    "\n",
    "In order to achieve quantization we can use an existing model architecture and simply create an other class adding only 2 pytorch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module): # Define a simple neural network\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, hidden_dim//2)\n",
    "        self.linear3 = torch.nn.Linear(hidden_dim//2, 10)\n",
    "        self.output = torch.nn.Linear(10, output_dim)\n",
    "\n",
    "    def forward(self, x): # Forward pass\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = torch.nn.functional.relu(self.linear2(x))\n",
    "        x = torch.nn.functional.relu(self.linear3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "class QuantizedModel(nn.Module): # Define a quantized model\n",
    "    def __init__(self, model):\n",
    "        super(QuantizedModel, self).__init__() # Define a quantized model using QuantStub, DeQuantStub\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x): # Forward pass using QuantStub, DeQuantStub to quantize and dequantize the input, output\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base functions\n",
    "We define the basics to generate data and to train the model, note that you need nothing more than usual here for the QAT to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(input_dim, output_dim, num_samples=100): # Generate random input and output data\n",
    "    np.random.seed(0)\n",
    "    inputs = np.random.rand(num_samples, input_dim).astype(np.float32) \n",
    "    targets = np.random.rand(num_samples, output_dim).astype(np.float32)\n",
    "    inputs = torch.from_numpy(inputs)\n",
    "    targets = torch.from_numpy(targets)\n",
    "    train_loader = torch.utils.data.DataLoader(list(zip(inputs, targets)), batch_size=10)\n",
    "    test_loader = torch.utils.data.DataLoader(list(zip(inputs, targets)), batch_size=10)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def training_loop(model, train_loader, test_loader, criterion, optimizer, num_epochs=10): # Training loop\n",
    "    model.train() # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader: # Train the model\n",
    "            optimizer.zero_grad() # Zero the gradients before the backward pass in order to avoid accumulating them\n",
    "            outputs = model(inputs) # Forward pass\n",
    "            loss = criterion(outputs, targets) # Compute the loss\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update the weights\n",
    "        #print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "        model.eval() \n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        #print(f\"Test Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Define the dimension of your model's layers, be aware that you need it to be large enough so that the quantization doesn't outweights the benefits of reduced computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 8 \n",
    "hidden_dim = 4000 \n",
    "output_dim = 1\n",
    "\n",
    "train_loader, test_loader = get_train_test_data(input_dim, output_dim) # Get the training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the fp32 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = Model(input_dim, hidden_dim, output_dim) # Create a model\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=0.01) # Create an optimizer\n",
    "criterion = nn.MSELoss() # Create a loss function\n",
    "\n",
    "training_loop(model_fp32, train_loader, test_loader, criterion, optimizer) # Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "We can now proceed to our model quantization (note that it works the same with a non pre-trained model).\n",
    "\n",
    "#### We only need 3 more lines and a calibration :\n",
    "\n",
    "-set the model qconfig (depending on the hardware your using, here fbgemm on Intel), which is the setup of default config for QAT (int8 etc..)\n",
    "\n",
    "-prepare the model to QAT which converts the model to quantized, applies qconfig, enable observers etc..\n",
    "\n",
    "-now we have to calibrate the model using sample inputs in order for it to find the quantization parameters\n",
    "\n",
    "now we can train / fine-tune our model\n",
    "\n",
    "-finally, we use the convert function which ends all the simulation stuff happening in training and gives us the trully quantized model\n",
    "\n",
    "note that the inplace flags are optionnal, they simply indicates if you want your model to be impacted or if it creates a copy of it (here, the same model is being modified again and again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_model(model, train_loader): # Calibrate the model to find the quantization parameters\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    for inputs, _ in train_loader: # Run the model on the training data\n",
    "        _ = model(inputs) # Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camille.hascoet/Documents/Green AI/.venv/lib/python3.12/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "model_to_quantize = copy.deepcopy(model_fp32) # Create a copy of the model to quantize\n",
    " \n",
    "model_int8_qat = QuantizedModel(model_to_quantize) # Create a quantized model\n",
    "model_int8_qat.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm') # Set the quantization configuration\n",
    "torch.quantization.prepare_qat(model_int8_qat, inplace=True) # Prepare the model for quantization\n",
    "optimizer_int8_qat = optim.Adam(model_int8_qat.parameters(), lr=0.01) # Create an optimizer for the quantized model\n",
    "\n",
    "calibrate_model(model_int8_qat, train_loader) # Calibrate the quantized model\n",
    "training_loop(model_int8_qat, train_loader, test_loader, criterion, optimizer_int8_qat) # Train the quantized model if needed\n",
    "\n",
    "model_int8 = torch.quantization.convert(model_int8_qat, inplace=True) # Convert the quantized model to int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Let's create a small function to calculate our model's inference time, and carbon emissions using the codecarbon library.\n",
    "\n",
    "We also create a small function to compare both model's sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare both models inference time\n",
    "import time\n",
    "import codecarbon\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "def inference_time(model, data_loader, num_iterations=100, warmup_iterations = 10):\n",
    "\n",
    "    tracker = EmissionsTracker(log_level='critical', save_to_file=False) # Initialize the co2 tracker\n",
    "    tracker.start() # Start the tracker\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    for _ in range(warmup_iterations): # Warmup the model in order to reduce the variance of the measurements\n",
    "        for inputs, targets in data_loader:\n",
    "            with torch.no_grad(): # Disable gradient tracking\n",
    "                model.forward(inputs)\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(num_iterations): # Measure the inference time based on multiple iterations to reduce the variance\n",
    "        for inputs, targets in data_loader: \n",
    "            with torch.no_grad(): # Disable gradient tracking\n",
    "                model.forward(inputs) \n",
    "    end = time.time()\n",
    "    total_emissions = tracker.stop() # Stop the tracker and get the total emissions\n",
    "\n",
    "    return (end - start) / num_iterations, total_emissions / num_iterations\n",
    "\n",
    "import os\n",
    "def get_model_size(mdl):\n",
    "    \n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\") # Save the model to disk\n",
    "    size = os.path.getsize(\"tmp.pt\") # Get the size of the model\n",
    "    os.remove('tmp.pt') # Remove the temporary file\n",
    "\n",
    "    return size\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    total_loss = 0 # Initialize the total loss\n",
    "    with torch.no_grad(): # Disable gradient tracking\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs) # Forward pass\n",
    "            loss = criterion(outputs, targets) # Compute the loss\n",
    "            total_loss += loss.item() # Accumulate the loss\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute everything we want to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_inference_time, fp32_emissions = inference_time(model_fp32, test_loader) # Measure the inference time of the FP32 model\n",
    "\n",
    "int8_inference_time, int8_emissions = inference_time(model_int8, test_loader) # Measure the inference time of the INT8 model\n",
    "\n",
    "fp32_size = get_model_size(model_fp32) # Get the size of the FP32 model\n",
    "\n",
    "int8_size = get_model_size(model_int8) # Get the size of the INT8 model\n",
    "\n",
    "fp32_loss = evaluate_model(model_fp32, test_loader, criterion) # Evaluate the FP32 model\n",
    "\n",
    "int8_loss = evaluate_model(model_int8, test_loader, criterion) # Evaluate the INT8 model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model  Inference Time (s)  Emissions (kgCO2)  Model Size (MB)      Loss\n",
      "0  FP32            0.023889       9.942014e-09         32234768  0.683736\n",
      "1  INT8            0.020083       7.959728e-09          8179664  0.298309\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({    \n",
    "    'Model': ['FP32', 'INT8'],\n",
    "    'Inference Time (s)': [fp32_inference_time, int8_inference_time],\n",
    "    'Emissions (kgCO2)': [fp32_emissions, int8_emissions],\n",
    "    'Model Size (MB)': [fp32_size, int8_size],\n",
    "    'Loss': [fp32_loss, int8_loss]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the results are better in very aspects ! Usually loss will decrease a very small amount but the gain elsewhere is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (linear1): Linear(in_features=8, out_features=4000, bias=True)\n",
      "  (linear2): Linear(in_features=4000, out_features=2000, bias=True)\n",
      "  (linear3): Linear(in_features=2000, out_features=10, bias=True)\n",
      "  (output): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizedModel(\n",
      "  (quant): Quantize(scale=tensor([0.0078]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (model): Model(\n",
      "    (linear1): QuantizedLinear(in_features=8, out_features=4000, scale=0.022292088717222214, zero_point=76, qscheme=torch.per_channel_affine)\n",
      "    (linear2): QuantizedLinear(in_features=4000, out_features=2000, scale=0.3334226906299591, zero_point=46, qscheme=torch.per_channel_affine)\n",
      "    (linear3): QuantizedLinear(in_features=2000, out_features=10, scale=0.6604927778244019, zero_point=127, qscheme=torch.per_channel_affine)\n",
      "    (output): QuantizedLinear(in_features=10, out_features=1, scale=0.00018686012481339276, zero_point=0, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_int8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
