{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Notebook to work on Spotify songs' popularity prediction while trying to have a Green AI approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librairies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import codecarbon\n",
    "from codecarbon import EmissionsTracker\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset, quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586672\n"
     ]
    }
   ],
   "source": [
    "spotify_songs = pd.read_csv('/Users/camille.hascoet/Documents/Green AI/Datasets/tracks.csv')\n",
    "spotify_songs.head()\n",
    "#number of rows\n",
    "dataset_length = len(spotify_songs)\n",
    "print(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>artists</th>\n",
       "      <th>id_artists</th>\n",
       "      <th>release_date</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93802</th>\n",
       "      <td>4iJyoBOLtHqaGxP12qzhQI</td>\n",
       "      <td>Peaches (feat. Daniel Caesar &amp; Giveon)</td>\n",
       "      <td>100</td>\n",
       "      <td>198082</td>\n",
       "      <td>1</td>\n",
       "      <td>['Justin Bieber', 'Daniel Caesar', 'Giveon']</td>\n",
       "      <td>['1uNFoZAHBGtllmzznpCI3s', '20wkVLutqVOYrc0kxF...</td>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.181</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.32100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.464</td>\n",
       "      <td>90.030</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93803</th>\n",
       "      <td>7lPN2DXiMsVn7XUKtOW1CS</td>\n",
       "      <td>drivers license</td>\n",
       "      <td>99</td>\n",
       "      <td>242014</td>\n",
       "      <td>1</td>\n",
       "      <td>['Olivia Rodrigo']</td>\n",
       "      <td>['1McMsnEElThX1knmY4oliG']</td>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.436</td>\n",
       "      <td>10</td>\n",
       "      <td>-8.761</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0601</td>\n",
       "      <td>0.72100</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>143.874</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93804</th>\n",
       "      <td>3Ofmpyhv5UAQ70mENzB277</td>\n",
       "      <td>Astronaut In The Ocean</td>\n",
       "      <td>98</td>\n",
       "      <td>132780</td>\n",
       "      <td>0</td>\n",
       "      <td>['Masked Wolf']</td>\n",
       "      <td>['1uU7g3DNSbsu0QjSEqZtEd']</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.695</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.865</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.472</td>\n",
       "      <td>149.996</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92810</th>\n",
       "      <td>5QO79kh1waicV47BqGRL3g</td>\n",
       "      <td>Save Your Tears</td>\n",
       "      <td>97</td>\n",
       "      <td>215627</td>\n",
       "      <td>1</td>\n",
       "      <td>['The Weeknd']</td>\n",
       "      <td>['1Xyo4u8uXC1ZmMpatF05PJ']</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.487</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.02120</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.5430</td>\n",
       "      <td>0.644</td>\n",
       "      <td>118.051</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92811</th>\n",
       "      <td>6tDDoYIxWvMLTdKpjFkc1B</td>\n",
       "      <td>telepatía</td>\n",
       "      <td>97</td>\n",
       "      <td>160191</td>\n",
       "      <td>0</td>\n",
       "      <td>['Kali Uchis']</td>\n",
       "      <td>['1U1el3k54VvEUzo3ybLPlM']</td>\n",
       "      <td>2020-12-04</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.524</td>\n",
       "      <td>11</td>\n",
       "      <td>-9.016</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.553</td>\n",
       "      <td>83.970</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93805</th>\n",
       "      <td>7MAibcTli4IisCtbHKrGMh</td>\n",
       "      <td>Leave The Door Open</td>\n",
       "      <td>96</td>\n",
       "      <td>242096</td>\n",
       "      <td>0</td>\n",
       "      <td>['Bruno Mars', 'Anderson .Paak', 'Silk Sonic']</td>\n",
       "      <td>['0du5cEVh5yTK9QJze8zA0C', '3jK9MiCrA42lLAdMGU...</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.616</td>\n",
       "      <td>5</td>\n",
       "      <td>-7.964</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.18200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.719</td>\n",
       "      <td>148.088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92813</th>\n",
       "      <td>0VjIjW4GlUZAMYd2vXMi3b</td>\n",
       "      <td>Blinding Lights</td>\n",
       "      <td>96</td>\n",
       "      <td>200040</td>\n",
       "      <td>0</td>\n",
       "      <td>['The Weeknd']</td>\n",
       "      <td>['1Xyo4u8uXC1ZmMpatF05PJ']</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.934</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>0.334</td>\n",
       "      <td>171.005</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92814</th>\n",
       "      <td>6f3Slt0GbA2bPZlz0aIFXN</td>\n",
       "      <td>The Business</td>\n",
       "      <td>95</td>\n",
       "      <td>164000</td>\n",
       "      <td>0</td>\n",
       "      <td>['Tiësto']</td>\n",
       "      <td>['2o5jDhtHVPhrJdv3cEQ99Z']</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.620</td>\n",
       "      <td>8</td>\n",
       "      <td>-7.079</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.41400</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.235</td>\n",
       "      <td>120.031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92816</th>\n",
       "      <td>3FAJ6O0NOHQV8Mc5Ri6ENp</td>\n",
       "      <td>Heartbreak Anniversary</td>\n",
       "      <td>94</td>\n",
       "      <td>198371</td>\n",
       "      <td>0</td>\n",
       "      <td>['Giveon']</td>\n",
       "      <td>['4fxd5Ee7UefO4CUXgwJ7IP']</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.964</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0791</td>\n",
       "      <td>0.52400</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3030</td>\n",
       "      <td>0.543</td>\n",
       "      <td>89.087</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92819</th>\n",
       "      <td>1xK1Gg9SxG8fy2Ya373oqb</td>\n",
       "      <td>Bandido</td>\n",
       "      <td>94</td>\n",
       "      <td>232853</td>\n",
       "      <td>0</td>\n",
       "      <td>['Myke Towers', 'Juhn']</td>\n",
       "      <td>['7iK8PXO48WeuP03g8YR51W', '2LmcxBak1alK1bf7d1...</td>\n",
       "      <td>2020-12-10</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.617</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.637</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.12200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.682</td>\n",
       "      <td>168.021</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id                                    name  \\\n",
       "93802  4iJyoBOLtHqaGxP12qzhQI  Peaches (feat. Daniel Caesar & Giveon)   \n",
       "93803  7lPN2DXiMsVn7XUKtOW1CS                         drivers license   \n",
       "93804  3Ofmpyhv5UAQ70mENzB277                  Astronaut In The Ocean   \n",
       "92810  5QO79kh1waicV47BqGRL3g                         Save Your Tears   \n",
       "92811  6tDDoYIxWvMLTdKpjFkc1B                               telepatía   \n",
       "93805  7MAibcTli4IisCtbHKrGMh                     Leave The Door Open   \n",
       "92813  0VjIjW4GlUZAMYd2vXMi3b                         Blinding Lights   \n",
       "92814  6f3Slt0GbA2bPZlz0aIFXN                            The Business   \n",
       "92816  3FAJ6O0NOHQV8Mc5Ri6ENp                  Heartbreak Anniversary   \n",
       "92819  1xK1Gg9SxG8fy2Ya373oqb                                 Bandido   \n",
       "\n",
       "       popularity  duration_ms  explicit  \\\n",
       "93802         100       198082         1   \n",
       "93803          99       242014         1   \n",
       "93804          98       132780         0   \n",
       "92810          97       215627         1   \n",
       "92811          97       160191         0   \n",
       "93805          96       242096         0   \n",
       "92813          96       200040         0   \n",
       "92814          95       164000         0   \n",
       "92816          94       198371         0   \n",
       "92819          94       232853         0   \n",
       "\n",
       "                                              artists  \\\n",
       "93802    ['Justin Bieber', 'Daniel Caesar', 'Giveon']   \n",
       "93803                              ['Olivia Rodrigo']   \n",
       "93804                                 ['Masked Wolf']   \n",
       "92810                                  ['The Weeknd']   \n",
       "92811                                  ['Kali Uchis']   \n",
       "93805  ['Bruno Mars', 'Anderson .Paak', 'Silk Sonic']   \n",
       "92813                                  ['The Weeknd']   \n",
       "92814                                      ['Tiësto']   \n",
       "92816                                      ['Giveon']   \n",
       "92819                         ['Myke Towers', 'Juhn']   \n",
       "\n",
       "                                              id_artists release_date  \\\n",
       "93802  ['1uNFoZAHBGtllmzznpCI3s', '20wkVLutqVOYrc0kxF...   2021-03-19   \n",
       "93803                         ['1McMsnEElThX1knmY4oliG']   2021-01-08   \n",
       "93804                         ['1uU7g3DNSbsu0QjSEqZtEd']   2021-01-06   \n",
       "92810                         ['1Xyo4u8uXC1ZmMpatF05PJ']   2020-03-20   \n",
       "92811                         ['1U1el3k54VvEUzo3ybLPlM']   2020-12-04   \n",
       "93805  ['0du5cEVh5yTK9QJze8zA0C', '3jK9MiCrA42lLAdMGU...   2021-03-05   \n",
       "92813                         ['1Xyo4u8uXC1ZmMpatF05PJ']   2020-03-20   \n",
       "92814                         ['2o5jDhtHVPhrJdv3cEQ99Z']   2020-09-16   \n",
       "92816                         ['4fxd5Ee7UefO4CUXgwJ7IP']   2020-03-27   \n",
       "92819  ['7iK8PXO48WeuP03g8YR51W', '2LmcxBak1alK1bf7d1...   2020-12-10   \n",
       "\n",
       "       danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "93802         0.677   0.696    0    -6.181     1       0.1190       0.32100   \n",
       "93803         0.585   0.436   10    -8.761     1       0.0601       0.72100   \n",
       "93804         0.778   0.695    4    -6.865     0       0.0913       0.17500   \n",
       "92810         0.680   0.826    0    -5.487     1       0.0309       0.02120   \n",
       "92811         0.653   0.524   11    -9.016     0       0.0502       0.11200   \n",
       "93805         0.586   0.616    5    -7.964     1       0.0324       0.18200   \n",
       "92813         0.514   0.730    1    -5.934     1       0.0598       0.00146   \n",
       "92814         0.798   0.620    8    -7.079     0       0.2320       0.41400   \n",
       "92816         0.449   0.465    0    -8.964     1       0.0791       0.52400   \n",
       "92819         0.713   0.617    8    -4.637     1       0.0887       0.12200   \n",
       "\n",
       "       instrumentalness  liveness  valence    tempo  time_signature  \n",
       "93802          0.000000    0.4200    0.464   90.030               4  \n",
       "93803          0.000013    0.1050    0.132  143.874               4  \n",
       "93804          0.000000    0.1500    0.472  149.996               4  \n",
       "92810          0.000012    0.5430    0.644  118.051               4  \n",
       "92811          0.000000    0.2030    0.553   83.970               4  \n",
       "93805          0.000000    0.0927    0.719  148.088               4  \n",
       "92813          0.000095    0.0897    0.334  171.005               4  \n",
       "92814          0.019200    0.1120    0.235  120.031               4  \n",
       "92816          0.000001    0.3030    0.543   89.087               3  \n",
       "92819          0.000000    0.0962    0.682  168.021               4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The 10 most popular songs\n",
    "spotify_songs.sort_values(by='popularity', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the features\n",
    "X = spotify_songs.drop(['popularity', 'id', 'name', 'artists', 'id_artists', 'release_date'], axis=1)\n",
    "y = spotify_songs['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loudness            0.327028\n",
       "energy              0.302315\n",
       "explicit            0.211758\n",
       "danceability        0.187000\n",
       "time_signature      0.086759\n",
       "tempo               0.071364\n",
       "duration_ms         0.027681\n",
       "key                 0.015299\n",
       "valence             0.004643\n",
       "mode               -0.033655\n",
       "speechiness        -0.047357\n",
       "liveness           -0.048740\n",
       "instrumentalness   -0.236487\n",
       "acousticness       -0.370882\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find correlation between features and target\n",
    "correlation = X.corrwith(y)\n",
    "correlation = correlation.sort_values(ascending=False)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop = X.drop(['mode', 'valence', 'key', 'duration_ms', 'speechiness', 'liveness'], axis=1)\n",
    "\n",
    "input_size = X_drop.shape[1]\n",
    "output_size = 1\n",
    "hidden_size = 50\n",
    "max_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 14:25:32.691548: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                450       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10000)             510000    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 400)               4000400   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 400)               160400    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 200)               80200     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4811851 (18.36 MB)\n",
      "Trainable params: 4811851 (18.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)),\n",
    "    tf.keras.layers.Dense(10000, activation='relu'),\n",
    "    tf.keras.layers.Dense(400, activation='relu'),\n",
    "    tf.keras.layers.Dense(400, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2757005277224752 0.2738955\n"
     ]
    }
   ],
   "source": [
    "#preprocessing features standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaler =StandardScaler()\n",
    "X_scaled = X_scaler.fit_transform(X_drop)\n",
    "\n",
    "y_scaled = y.values.reshape(-1,1) / 100\n",
    "\n",
    "#shuffle and take 20000 samples\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_shuffled, y_shuffled = shuffle(X_scaled, y_scaled, random_state=1234)\n",
    "\n",
    "X_train = X_shuffled[:20000]\n",
    "y_train = y_shuffled[:20000]\n",
    "\n",
    "X_test = X_shuffled[20000:25000]\n",
    "y_test = y_shuffled[20000:25000]\n",
    "\n",
    "print(y_scaled.mean(), y_train.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 [==============================] - 58s 54ms/step - loss: 0.1337 - val_loss: 0.1315\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 35s 55ms/step - loss: 0.1282 - val_loss: 0.1296\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.1270 - val_loss: 0.1274\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.1261 - val_loss: 0.1265\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.1253 - val_loss: 0.1265\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.1249 - val_loss: 0.1277\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.1243 - val_loss: 0.1283\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.1240 - val_loss: 0.1273\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.1234 - val_loss: 0.1267\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.1233 - val_loss: 0.1261\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.1229 - val_loss: 0.1259\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.1227 - val_loss: 0.1254\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.1225 - val_loss: 0.1264\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.1224 - val_loss: 0.1265\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.1222 - val_loss: 0.1263\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.1217 - val_loss: 0.1255\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.1219 - val_loss: 0.1284\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 35s 55ms/step - loss: 0.1215 - val_loss: 0.1257\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.1215 - val_loss: 0.1279\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.1213 - val_loss: 0.1275\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.1210 - val_loss: 0.1265\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.1207 - val_loss: 0.1258\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.1205 - val_loss: 0.1261\n",
      "Epoch 24/30\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.1200 - val_loss: 0.1259\n",
      "Epoch 25/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1199 - val_loss: 0.1266\n",
      "Epoch 26/30\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.1197 - val_loss: 0.1260\n",
      "Epoch 27/30\n",
      "625/625 [==============================] - 34s 55ms/step - loss: 0.1193 - val_loss: 0.1254\n",
      "Epoch 28/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1196 - val_loss: 0.1263\n",
      "Epoch 29/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1192 - val_loss: 0.1257\n",
      "Epoch 30/30\n",
      "625/625 [==============================] - 37s 60ms/step - loss: 0.1185 - val_loss: 0.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: EnergyDriver_executeCommands [via readSample] returned 0xe00002bc\n",
      "ERROR: EnergyDriver_executeCommands [via readSample] returned 0xe00002bc\n"
     ]
    }
   ],
   "source": [
    "tracker = EmissionsTracker(log_level='error')\n",
    "tracker.start()\n",
    "\n",
    "model.fit(X_train, y_train, epochs=max_epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "total_emissions_normal = tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emissions for normal model: 0.0354 gCO2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total emissions for normal model: {total_emissions_normal*100:.4f} gCO2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 50)                450       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10000)             510000    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 400)               4000400   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 400)               160400    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 200)               80200     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4811851 (18.36 MB)\n",
      "Trainable params: 4811851 (18.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLa  (None, 8)                 3         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " quant_dense_8 (QuantizeWra  (None, 50)                455       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_9 (QuantizeWra  (None, 10000)             510005    \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_10 (QuantizeWr  (None, 400)               4000405   \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_11 (QuantizeWr  (None, 400)               160405    \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_12 (QuantizeWr  (None, 200)               80205     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_13 (QuantizeWr  (None, 200)               40205     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_14 (QuantizeWr  (None, 100)               20105     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_15 (QuantizeWr  (None, 1)                 106       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4811894 (18.36 MB)\n",
      "Trainable params: 4811851 (18.36 MB)\n",
      "Non-trainable params: 43 (172.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "model_to_quantize = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)),\n",
    "    tf.keras.layers.Dense(10000, activation='relu'),\n",
    "    tf.keras.layers.Dense(400, activation='relu'),\n",
    "    tf.keras.layers.Dense(400, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='linear')\n",
    "])\n",
    "model_to_quantize.summary()\n",
    "q_aware_model = quantize_model(model_to_quantize)\n",
    "\n",
    "q_aware_model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 [==============================] - 47s 67ms/step - loss: 0.1337\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.1279\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 37s 60ms/step - loss: 0.1268\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.1258\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 40s 63ms/step - loss: 0.1253\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.1250\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 37s 60ms/step - loss: 0.1243\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.1243\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.1241\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.1231\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.1229\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.1229\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.1222\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1221\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1218\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 38s 60ms/step - loss: 0.1217\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.1214\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1215\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.1209\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1207\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1204\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1198\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 37s 58ms/step - loss: 0.1198\n",
      "Epoch 24/30\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.1195\n",
      "Epoch 25/30\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.1190\n",
      "Epoch 26/30\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1187\n",
      "Epoch 27/30\n",
      "625/625 [==============================] - 39s 62ms/step - loss: 0.1184\n",
      "Epoch 28/30\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.1185\n",
      "Epoch 29/30\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.1180\n",
      "Epoch 30/30\n",
      "625/625 [==============================] - 37s 60ms/step - loss: 0.1174\n"
     ]
    }
   ],
   "source": [
    "tracker.start()\n",
    "q_aware_model.fit(X_train, y_train, epochs=max_epochs)\n",
    "total_emissions_qaware = tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emissions for quantized aware model: 0.7261 gCO2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total emissions for quantized aware model: {total_emissions_qaware*1000:.4f} gCO2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare inference time and precision\n",
    "import time\n",
    "\n",
    "def inference_time_and_precision(model, X, y_test, loss):\n",
    "    start = time.time()\n",
    "    #model evaluate function\n",
    "    loss_value = model.evaluate(X, y_test)\n",
    "    end = time.time()\n",
    "\n",
    "    inference_time = end - start\n",
    "    \n",
    "    return inference_time, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 3s 12ms/step - loss: 0.1275\n",
      "Inference time for quantized aware model: 3.4459 seconds\n",
      "Loss for quantized aware model: 0.1275\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 0.1259\n",
      "Inference time for normal model: 1.1066 seconds\n",
      "Loss for normal model: 0.1259\n"
     ]
    }
   ],
   "source": [
    "qat_inference_time, qat_loss = inference_time_and_precision(q_aware_model, X_test, y_test, tf.keras.losses.MeanAbsoluteError())\n",
    "\n",
    "print(f\"Inference time for quantized aware model: {qat_inference_time:.4f} seconds\")\n",
    "\n",
    "print(f\"Loss for quantized aware model: {qat_loss:.4f}\")\n",
    "\n",
    "normal_inference_time, normal_loss = inference_time_and_precision(model, X_test, y_test, tf.keras.losses.MeanAbsoluteError())\n",
    "\n",
    "print(f\"Inference time for normal model: {normal_inference_time:.4f} seconds\")\n",
    "\n",
    "print(f\"Loss for normal model: {normal_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>emissions</th>\n",
       "      <th>inference_time (5000 samples, sec)</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normal</td>\n",
       "      <td>0.353644</td>\n",
       "      <td>1.106617</td>\n",
       "      <td>0.125856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quantized aware</td>\n",
       "      <td>0.726065</td>\n",
       "      <td>3.445926</td>\n",
       "      <td>0.127549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  emissions  inference_time (5000 samples, sec)      loss\n",
       "0           normal   0.353644                            1.106617  0.125856\n",
       "1  quantized aware   0.726065                            3.445926  0.127549"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe of the results\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'model': ['normal', 'quantized aware'],\n",
    "    'emissions': [total_emissions_normal*1000, total_emissions_qaware*1000],\n",
    "    'inference_time (5000 samples, sec)': [normal_inference_time, qat_inference_time],\n",
    "    'loss': [normal_loss, qat_loss]\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 1000)              9000      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 1001      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2012001 (7.68 MB)\n",
      "Trainable params: 2012001 (7.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#bigger model\n",
    "hidden_size_1 = 1000\n",
    "hidden_size_2 = 1000\n",
    "hidden_size_3 = 1000\n",
    "max_epochs_bigger = 10\n",
    "\n",
    "model_bigger = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_size_1, activation='relu', input_shape=(input_size,)),\n",
    "    tf.keras.layers.Dense(hidden_size_2, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_size_3, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "model_bigger.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "model_bigger.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_1 (Quantize  (None, 8)                 3         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " quant_dense_20 (QuantizeWr  (None, 1000)              9005      \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_21 (QuantizeWr  (None, 1000)              1001005   \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_22 (QuantizeWr  (None, 1000)              1001005   \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_23 (QuantizeWr  (None, 1)                 1006      \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2012024 (7.68 MB)\n",
      "Trainable params: 2012001 (7.68 MB)\n",
      "Non-trainable params: 23 (92.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bigger_quantize = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_size_1, activation='relu', input_shape=(input_size,)),\n",
    "    tf.keras.layers.Dense(hidden_size_2, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_size_3, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model_bigger = quantize_model(model_bigger_quantize)\n",
    "\n",
    "q_aware_model_bigger.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "q_aware_model_bigger.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 11s 14ms/step - loss: 0.1344\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1270\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.1259\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.1251\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1249\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1246\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1239\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.1238\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.1235\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.1232\n",
      "Total emissions for normal bigger model: 0.7546 gCO2\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 12s 14ms/step - loss: 0.1360\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.1275\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.1264\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.1254\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.1250\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1251\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.1239\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.1241\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1234\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.1234\n",
      "Total emissions for quantized aware bigger model: 0.7855 gCO2\n",
      "157/157 [==============================] - 2s 6ms/step - loss: 0.1275\n",
      "Inference time for quantized aware bigger model: 1.8173 seconds\n",
      "Loss for quantized aware bigger model: 0.1275\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.1261\n",
      "Inference time for normal bigger model: 1.3413 seconds\n",
      "Loss for normal bigger model: 0.1261\n"
     ]
    }
   ],
   "source": [
    "tracker.start()\n",
    "model_bigger.fit(X_train, y_train, epochs=max_epochs_bigger)\n",
    "total_emissions_normal_bigger = tracker.stop()\n",
    "\n",
    "print(f\"Total emissions for normal bigger model: {total_emissions_normal_bigger*1000:.4f} gCO2\")\n",
    "\n",
    "tracker.start()\n",
    "q_aware_model_bigger.fit(X_train, y_train, epochs=max_epochs_bigger)\n",
    "total_emissions_qaware_bigger = tracker.stop()\n",
    "\n",
    "print(f\"Total emissions for quantized aware bigger model: {total_emissions_qaware_bigger*1000:.4f} gCO2\")\n",
    "\n",
    "qat_inference_time_bigger, qat_loss_bigger = inference_time_and_precision(q_aware_model_bigger, X_test, y_test, tf.keras.losses.MeanAbsoluteError())\n",
    "\n",
    "print(f\"Inference time for quantized aware bigger model: {qat_inference_time_bigger:.4f} seconds\")\n",
    "\n",
    "print(f\"Loss for quantized aware bigger model: {qat_loss_bigger:.4f}\")\n",
    "\n",
    "normal_inference_time_bigger, normal_loss_bigger = inference_time_and_precision(model_bigger, X_test, y_test, tf.keras.losses.MeanAbsoluteError())\n",
    "\n",
    "print(f\"Inference time for normal bigger model: {normal_inference_time_bigger:.4f} seconds\")\n",
    "\n",
    "print(f\"Loss for normal bigger model: {normal_loss_bigger:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>emissions</th>\n",
       "      <th>inference_time (5000 samples, sec)</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normal bigger</td>\n",
       "      <td>0.754577</td>\n",
       "      <td>1.341342</td>\n",
       "      <td>0.126140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quantized aware bigger</td>\n",
       "      <td>0.785458</td>\n",
       "      <td>1.817299</td>\n",
       "      <td>0.127521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  emissions  inference_time (5000 samples, sec)  \\\n",
       "0           normal bigger   0.754577                            1.341342   \n",
       "1  quantized aware bigger   0.785458                            1.817299   \n",
       "\n",
       "       loss  \n",
       "0  0.126140  \n",
       "1  0.127521  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigger_results = pd.DataFrame({\n",
    "    'model': ['normal bigger', 'quantized aware bigger'],\n",
    "    'emissions': [total_emissions_normal_bigger*1000, total_emissions_qaware_bigger*1000],\n",
    "    'inference_time (5000 samples, sec)': [normal_inference_time_bigger, qat_inference_time_bigger],\n",
    "    'loss': [normal_loss_bigger, qat_loss_bigger]\n",
    "})\n",
    "\n",
    "bigger_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal model size: 57807552 bytes\n",
      "Quantized aware model size: 57842896 bytes\n"
     ]
    }
   ],
   "source": [
    "#check the size of the models\n",
    "import os\n",
    "\n",
    "def get_model_size(model):\n",
    "    model.save('temp_model.h5')\n",
    "    size = os.path.getsize('temp_model.h5')\n",
    "    os.remove('temp_model.h5')\n",
    "    return size\n",
    "\n",
    "normal_model_size = get_model_size(model)\n",
    "\n",
    "print(f\"Normal model size: {normal_model_size} bytes\")\n",
    "\n",
    "q_aware_model_size = get_model_size(q_aware_model)\n",
    "\n",
    "print(f\"Quantized aware model size: {q_aware_model_size} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
