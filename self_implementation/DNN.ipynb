{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual implementation of a Neural Network\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import codecarbon\n",
    "import time\n",
    "import pprint\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN():\n",
    "    def __init__(self, input_size, output_size, layers = [], activations = [], learning_rate = 0.1, loss = 'mean_squared_error'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = layers\n",
    "        self.activations_functions = []\n",
    "        self.loss_function = lambda x, y: np.mean(np.square(x - y))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.num_layers = len(layers)\n",
    "        self.initialize_weights_and_biases()\n",
    "        self.set_activations_and_loss(activations, loss)\n",
    "\n",
    "    def set_activations_and_loss(self, activations, loss):\n",
    "        if len(activations) == 0:\n",
    "            activations = ['sigmoid'] * (len(self.layers) + 1)\n",
    "        elif len(activations) != len(self.layers) + 1:\n",
    "            raise ValueError('Number of activations must be equal to the number of layers + 1')\n",
    "        \n",
    "        self.activations = activations\n",
    "        self.activations_functions = [None] * len(activations)\n",
    "        for i in range(len(activations)):\n",
    "            if isinstance(activations[i], str):\n",
    "                if activations[i] == 'sigmoid':\n",
    "                    self.activations_functions[i] = lambda x: 1 / (1 + np.exp(-x))\n",
    "                elif activations[i] == 'relu':\n",
    "                    self.activations_functions[i] = lambda x: np.maximum(0, x)\n",
    "                elif activations[i] == 'tanh':\n",
    "                    self.activations_functions[i] = lambda x: np.tanh(x)\n",
    "                elif activations[i] == 'none':\n",
    "                    self.activations_functions[i] = lambda x: x\n",
    "                elif activations[i] == 'softmax':\n",
    "                    self.activations_functions[i] = lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "                else:\n",
    "                    raise ValueError('Activation function must be a string, supported functions are: sigmoid, relu, tanh, softmax, none')\n",
    "            else:\n",
    "                raise ValueError('Activation function must be a string, supported functions are: sigmoid, relu, tanh, softmax, none')\n",
    "        \n",
    "        self.derivatives = [None] * len(activations)\n",
    "        for i in range(len(activations)):\n",
    "            if activations[i] == 'sigmoid':\n",
    "                self.derivatives[i] = lambda x: (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x))))\n",
    "            elif activations[i] == 'relu':\n",
    "                self.derivatives[i] = lambda x: [1 if j > 0 else 0 for j in x]\n",
    "            elif activations[i] == 'tanh':\n",
    "                self.derivatives[i] = lambda x: 1 - np.tanh(x)**2\n",
    "            elif activations[i] == 'none':\n",
    "                self.derivatives[i] = lambda x: 1\n",
    "            elif activations[i] == 'softmax':\n",
    "                self.derivatives[i] = lambda x: x * (1 - x)\n",
    "            else:\n",
    "                raise ValueError('Activation function must be a string, supported functions are: sigmoid, relu, tanh, softmax, none')\n",
    "\n",
    "        if isinstance(loss, str):\n",
    "            if loss == 'mean_squared_error':\n",
    "                self.loss_function = lambda x, y: np.mean(np.square(x - y))\n",
    "                self.loss_derivative = lambda x, y: 2 * (x - y)\n",
    "            elif loss == 'binary_crossentropy':\n",
    "                self.loss_function = lambda x, y: -np.mean(y * np.log(x) + (1 - y) * np.log(1 - x))\n",
    "                self.loss_derivative = lambda x, y: (x - y) / (x * (1 - x))\n",
    "            elif loss == 'categorical_crossentropy':\n",
    "                self.loss_function = lambda x, y: -np.mean(y * np.log(x))\n",
    "                self.loss_derivative = lambda x, y: x - y\n",
    "            elif loss == 'crossentropy':\n",
    "                self.loss_function = lambda x, y: -np.mean(y * np.log(x))\n",
    "                self.loss_derivative = lambda x, y: x - y   \n",
    "            elif loss == 'mean_absolute_error':\n",
    "                self.loss_function = lambda x, y: np.mean(np.abs(x - y))\n",
    "                self.loss_derivative = lambda x, y: np.sign(x - y)\n",
    "            else:\n",
    "                raise ValueError('Loss function must be a string, supported functions are: mean_squared_error, binary_crossentropy, categorical_crossentropy, mean_absolute_error, crossentropy')\n",
    "        else:\n",
    "            raise ValueError('Loss function must be a string, supported functions are: mean_squared_error, binary_crossentropy, categorical_crossentropy, mean_absolute_error, crossentropy')\n",
    "        \n",
    "        self.loss = loss\n",
    "\n",
    "    def initialize_weights_and_biases(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            if i == 0:\n",
    "                #random in range -0.5, 0.5\n",
    "                self.weights.append(np.random.rand(self.input_size, self.layers[i]) - 0.5)\n",
    "                self.biases.append(np.random.rand(self.layers[i]) - 0.5)\n",
    "            else:\n",
    "                self.weights.append(np.random.rand(self.layers[i-1], self.layers[i]) - 0.5)\n",
    "                self.biases.append(np.random.rand(self.layers[i]) - 0.5)\n",
    "        self.weights.append(np.random.rand(self.layers[-1], self.output_size) - 0.5)\n",
    "        self.biases.append(np.random.rand(self.output_size) - 0.5)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = []\n",
    "        self.a = []\n",
    "        for i in range(len(self.weights)):\n",
    "            if i == 0:\n",
    "                self.z.append(np.dot(X, self.weights[i]) + self.biases[i])\n",
    "            else:\n",
    "                self.z.append(np.dot(self.a[i-1], self.weights[i]) + self.biases[i])\n",
    "            self.a.append(self.activations_functions[i](self.z[i]))\n",
    "        return self.a[-1]\n",
    "    \n",
    "    def backward(self, X, y, y_pred):\n",
    "        self.deltas = [None] * (self.num_layers + 1)\n",
    "        error = y_pred - np.asarray(y).reshape(-1, 1)\n",
    "        mean_error = np.mean(error, axis=0)\n",
    "        for i in range(self.num_layers, -1, -1): # need to handle batches\n",
    "            self.deltas[i] = (mean_error * self.derivatives[i](self.z[i].mean(axis = 0))) if i == self.num_layers else self.compute_deltas(i, X)\n",
    "    \n",
    "    def compute_deltas(self, index, X):\n",
    "        return (np.dot(self.deltas[index+1], self.weights[index+1].T) * self.derivatives[index](self.z[index].mean(axis = 0)))\n",
    "\n",
    "    def update_weights_and_biases(self, inputs):\n",
    "        for k in range(len(self.weights)-1):\n",
    "            for i in range(self.weights[k].shape[0]-1):\n",
    "                for j in range(self.weights[k].shape[1]-1):                \n",
    "                    self.weights[k][i][j] -= (self.learning_rate * self.deltas[k].T[j] * self.a[k-1][:,i].mean() if k != 0 else self.learning_rate * self.deltas[k].T[j] * inputs.T[i].mean(axis=0))\n",
    "                    self.biases[k][j] -= self.learning_rate * self.deltas[k].T[j]\n",
    "        \n",
    "    def fit(self, X, y, epochs = 30, batch_size = 32):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, len(X), self.batch_size):\n",
    "                try:\n",
    "                    X_batch = X[i:i+self.batch_size]\n",
    "                    y_batch = y[i:i+self.batch_size]\n",
    "                except:\n",
    "                    X_batch = X[i:]\n",
    "                    y_batch = y[i:]\n",
    "                y_pred = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch, y_pred)\n",
    "                self.update_weights_and_biases(X_batch)\n",
    "            print(f'Epoch {epoch+1}/{self.epochs} - Loss: {self.evaluate(X, y)}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.forward(X)\n",
    "        y = np.asarray(y).reshape(-1, 1)\n",
    "        return self.loss_function(y_pred, y)\n",
    "    \n",
    "    def summary(self):\n",
    "        print('Model Summary')\n",
    "        print('Input Size:', self.input_size)\n",
    "        print('Output Size:', self.output_size)\n",
    "        print('Layers:', self.layers)\n",
    "        print('Activations:', [self.activations[i] for i in range(len(self.activations))])\n",
    "        print('Loss:', self.loss)\n",
    "        print('Learning Rate:', self.learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spotify data & preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loudness            0.327028\n",
       "energy              0.302315\n",
       "explicit            0.211758\n",
       "danceability        0.187000\n",
       "time_signature      0.086759\n",
       "tempo               0.071364\n",
       "duration_ms         0.027681\n",
       "key                 0.015299\n",
       "valence             0.004643\n",
       "mode               -0.033655\n",
       "speechiness        -0.047357\n",
       "liveness           -0.048740\n",
       "instrumentalness   -0.236487\n",
       "acousticness       -0.370882\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_songs = pd.read_csv('/Users/camille.hascoet/Documents/Green AI/Datasets/tracks.csv')\n",
    "X = spotify_songs.drop(['popularity', 'id', 'name', 'artists', 'id_artists', 'release_date'], axis=1)\n",
    "y = spotify_songs['popularity']\n",
    "correlation = X.corrwith(y).sort_values(ascending=False)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "test_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop = X.drop(['mode', 'valence', 'key', 'duration_ms', 'speechiness', 'liveness'], axis=1)\n",
    "X_drop = X_drop.fillna(X_drop.mean())\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaler = StandardScaler()\n",
    "X_scaled = X_scaler.fit_transform(X_drop)\n",
    "y_scaled = y / 100\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_shuffled, y_shuffled = shuffle(X_scaled, y_scaled, random_state=282)\n",
    "\n",
    "X_train, X_test = X_shuffled[:training_size], X_shuffled[training_size:training_size+test_size]\n",
    "y_train, y_test = y_shuffled[:training_size], y_shuffled[training_size:training_size+test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.274065 0.2757005277224752\n"
     ]
    }
   ],
   "source": [
    "print(y_train.mean(), y_scaled.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(input_size=8, output_size=1, layers=[50, 10], activations=['relu', 'relu', 'none'], learning_rate=0.01, loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 0.2135477996751101\n",
      "Epoch 2/30 - Loss: 0.19907510299888218\n",
      "Epoch 3/30 - Loss: 0.19217312692425134\n",
      "Epoch 4/30 - Loss: 0.18814100910319356\n",
      "Epoch 5/30 - Loss: 0.18533838501061087\n",
      "Epoch 6/30 - Loss: 0.1830499638776352\n",
      "Epoch 7/30 - Loss: 0.1812824083293911\n",
      "Epoch 8/30 - Loss: 0.17974270783642352\n",
      "Epoch 9/30 - Loss: 0.1784245804181317\n",
      "Epoch 10/30 - Loss: 0.17718902124583916\n",
      "Epoch 11/30 - Loss: 0.17589182866881173\n",
      "Epoch 12/30 - Loss: 0.1747352431291355\n",
      "Epoch 13/30 - Loss: 0.17352006202713843\n",
      "Epoch 14/30 - Loss: 0.17252868912938238\n",
      "Epoch 15/30 - Loss: 0.17153665941058052\n",
      "Epoch 16/30 - Loss: 0.17055643673725618\n",
      "Epoch 17/30 - Loss: 0.16965660594962007\n",
      "Epoch 18/30 - Loss: 0.16878463315764772\n",
      "Epoch 19/30 - Loss: 0.16792394257072327\n",
      "Epoch 20/30 - Loss: 0.167115288676454\n",
      "Epoch 21/30 - Loss: 0.16637580859637532\n",
      "Epoch 22/30 - Loss: 0.16563635193270887\n",
      "Epoch 23/30 - Loss: 0.16506924063506542\n",
      "Epoch 24/30 - Loss: 0.16461624049904125\n",
      "Epoch 25/30 - Loss: 0.16399824705134516\n",
      "Epoch 26/30 - Loss: 0.16340863067040182\n",
      "Epoch 27/30 - Loss: 0.16283371678483746\n",
      "Epoch 28/30 - Loss: 0.16239520523523007\n",
      "Epoch 29/30 - Loss: 0.16178045922271717\n",
      "Epoch 30/30 - Loss: 0.16121158189800955\n",
      "Training Time: 471.5015561580658\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=10)\n",
    "end_time = time.time()\n",
    "print('Training Time:', end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
